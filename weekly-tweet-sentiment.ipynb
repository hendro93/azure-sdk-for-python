{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hendrasbmitb/weekly-tweet-sentiment?scriptVersionId=128000238\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"a0f6b751","metadata":{"_cell_guid":"84bda12a-134e-4a5b-ac2f-301fb66e376e","_uuid":"5efa65ed-e03c-4f3f-a752-6fa8b046ed53","papermill":{"duration":0.006431,"end_time":"2023-05-02T08:06:39.655977","exception":false,"start_time":"2023-05-02T08:06:39.649546","status":"completed"},"tags":[]},"source":["# Weekly Tweet Sentiment\n","\n","\n","\n","This notebook is based on Jessica Garson's step-by-step tutorial on her blog, [**How to analyze the sentiment of your own Tweets**](https://developer.twitter.com/en/blog/community/2020/how-to-analyze-the-sentiment-of-your-own-tweets). This is the [**complete code**](https://github.com/hendro93/weekly-tweet-sentiment).\n","\n","\n","Setting up\n","\n","Before you can get started you will need to make sure you have the following:\n","\n","- Python 3 [**installed**](https://wiki.python.org/moin/BeginnersGuide/Download)\n","- Twitter Developer account: if you don’t have one already, you can [**apply for one**].(https://developer.twitter.com/en/portal/dashboard)\n","- [**A Twitter developer app**](https://developer.twitter.com/en/docs/apps/overview), which can be created in your Twitter developer account. \n","- A [**bearer token**](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens) for your app\n","- An account with Microsoft Azure’s [**Text Analytics Cognitive Service**](https://azure.microsoft.com/en-us/products/cognitive-services/text-analytics/) and an endpoint created. You can check out Microsoft’s [**quick start guide on how to call the Text Analytics API**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/overview).\n","\n","You will also need to install the library [**Requests**](https://requests.readthedocs.io/en/latest/). Requests will be used to make HTTP requests to the Twitter and Azure endpoints and pandas which is used to shape the data."]},{"cell_type":"code","execution_count":1,"id":"5e4dd5d3","metadata":{"execution":{"iopub.execute_input":"2023-05-02T08:06:39.668322Z","iopub.status.busy":"2023-05-02T08:06:39.667892Z","iopub.status.idle":"2023-05-02T08:06:53.248917Z","shell.execute_reply":"2023-05-02T08:06:53.247109Z"},"papermill":{"duration":13.590813,"end_time":"2023-05-02T08:06:53.252046","exception":false,"start_time":"2023-05-02T08:06:39.661233","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting azure-ai-textanalytics\r\n","  Downloading azure_ai_textanalytics-5.3.0b2-py3-none-any.whl (321 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from azure-ai-textanalytics) (4.4.0)\r\n","Collecting isodate<1.0.0,>=0.6.1\r\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting azure-core<2.0.0,>=1.24.0\r\n","  Downloading azure_core-1.26.4-py3-none-any.whl (173 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting azure-common~=1.1\r\n","  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\r\n","Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\r\n","Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.7/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.28.2)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.1.1)\r\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.26.14)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2022.12.7)\r\n","Installing collected packages: azure-common, isodate, azure-core, azure-ai-textanalytics\r\n","Successfully installed azure-ai-textanalytics-5.3.0b2 azure-common-1.1.28 azure-core-1.26.4 isodate-0.6.1\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["!pip install azure-ai-textanalytics --pre"]},{"cell_type":"code","execution_count":2,"id":"2d2c81cb","metadata":{"_cell_guid":"5f8228f2-bd56-4bcd-95ab-e6ffd7554529","_uuid":"725e03d2-1180-4660-a46f-e7f7f92db669","collapsed":false,"execution":{"iopub.execute_input":"2023-05-02T08:06:53.267099Z","iopub.status.busy":"2023-05-02T08:06:53.266628Z","iopub.status.idle":"2023-05-02T08:06:54.019246Z","shell.execute_reply":"2023-05-02T08:06:54.017755Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.764447,"end_time":"2023-05-02T08:06:54.022578","exception":false,"start_time":"2023-05-02T08:06:53.258131","status":"completed"},"tags":[]},"outputs":[],"source":["import requests\n","import json\n","import ast\n","\n","from azure.core.credentials import AzureKeyCredential\n","from azure.ai.textanalytics import TextAnalyticsClient\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","azure_endpoint = user_secrets.get_secret(\"AZURE_LANGUAGE_ENDPOINT\")\n","azure_language_key = user_secrets.get_secret(\"AZURE_LANGUAGE_KEY\")\n","bearer_token = user_secrets.get_secret(\"bearer_token\")"]},{"cell_type":"markdown","id":"97aa9f04","metadata":{"_cell_guid":"a09a4417-9841-41b0-91da-bab7d0b2b5c8","_uuid":"683998eb-272b-4acd-8f1a-f177649493b1","papermill":{"duration":0.005303,"end_time":"2023-05-02T08:06:54.033556","exception":false,"start_time":"2023-05-02T08:06:54.028253","status":"completed"},"tags":[]},"source":["## Creating the URL\n","\n","Before you can connect the Twitter API, you’ll need to set up the URL to ensure it has the right fields so you get the right data back. You’ll first need to create a function called  create_twitter_url in this function you’ll declare a variable for your handle, you can replace jessicagarson with your own handle (I changed the query format to a keyword which can be anything so it doesn't have to be a Twitter ID). The max_results can be anywhere from 1 to 100 (but keep in mind that Azure Sentiment Analysis will only process a maximum of 10 records per request). If you are using a handle that would have more than 100 Tweets in a given week you may want to build in some logic to handle pagination or use a library such as [**searchtweets-labs**](https://github.com/twitterdev/search-tweets-python). The URL will need to be formatted to contain the max number of results and the query to say that you are looking for Tweets from a specific handle. You’ll return the formatted URL in a variable called url, since you will need it to make a get GET request later.\n","\n","Note the difference in how the most recent Twitter API urls are written!"]},{"cell_type":"code","execution_count":3,"id":"d502534a","metadata":{"_cell_guid":"69208944-04f4-46d2-8955-46b285e35dc9","_uuid":"26f3ea5c-055d-4638-93f4-4c1013d8a901","collapsed":false,"execution":{"iopub.execute_input":"2023-05-02T08:06:54.04666Z","iopub.status.busy":"2023-05-02T08:06:54.046165Z","iopub.status.idle":"2023-05-02T08:06:54.052721Z","shell.execute_reply":"2023-05-02T08:06:54.051433Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016095,"end_time":"2023-05-02T08:06:54.055204","exception":false,"start_time":"2023-05-02T08:06:54.039109","status":"completed"},"tags":[]},"outputs":[],"source":["def create_twitter_url():\n","    handle = \"nasi goreng\"\n","    max_results = 10\n","    mrf = \"max_results={}\".format(max_results)\n","    q = \"query={}\".format(handle)\n","    url = \"https://api.twitter.com/2/tweets/search/recent?{}&{}\".format(\n","        mrf, q\n","    )\n","    return url"]},{"cell_type":"markdown","id":"1d92b2e5","metadata":{"_cell_guid":"ebd133b4-2c96-4456-81c7-d9a2ac86861b","_uuid":"22beb0e6-936c-43c9-83b4-a65bc7e5edf0","papermill":{"duration":0.005505,"end_time":"2023-05-02T08:06:54.066412","exception":false,"start_time":"2023-05-02T08:06:54.060907","status":"completed"},"tags":[]},"source":["The URL you are creating is:"]},{"cell_type":"code","execution_count":4,"id":"a3382aef","metadata":{"execution":{"iopub.execute_input":"2023-05-02T08:06:54.080042Z","iopub.status.busy":"2023-05-02T08:06:54.079615Z","iopub.status.idle":"2023-05-02T08:06:54.088454Z","shell.execute_reply":"2023-05-02T08:06:54.087065Z"},"papermill":{"duration":0.018772,"end_time":"2023-05-02T08:06:54.090933","exception":false,"start_time":"2023-05-02T08:06:54.072161","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'https://api.twitter.com/2/tweets/search/recent?max_results=10&query=nasi goreng'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["create_twitter_url()"]},{"cell_type":"markdown","id":"22424047","metadata":{"_cell_guid":"cdfd905e-940d-42d9-b7a4-354bcb0f2a38","_uuid":"0a5d197e-d40b-4e2f-a6aa-dc70d3226b18","papermill":{"duration":0.005576,"end_time":"2023-05-02T08:06:54.102385","exception":false,"start_time":"2023-05-02T08:06:54.096809","status":"completed"},"tags":[]},"source":["You can adjust your query if you wanted to exclude retweets or Tweets that contain media. You can make adjustments to the data that is returned by the Twitter API by adding additional fields and expansions to your query. Using a REST client such as [**Postman**](https://www.postman.com/) or [**Insomnia**](https://insomnia.rest/) can be helpful for seeing what data you get back and making adjustments before you start writing code. There is [**a Postman collection for Labs endpoints**](https://app.getpostman.com/run-collection/c3c275c6ea02c49c3311#?env%5BTwitter%20Developer%20Labs%5D=W3sia2V5IjoiY29uc3VtZXJfa2V5IiwidmFsdWUiOiJZb3VyIGNvbnN1bWVyIGtleSIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiY29uc3VtZXJfc2VjcmV0IiwidmFsdWUiOiJZb3VyIGNvbnN1bWVyIHNlY3JldCIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiYWNjZXNzX3Rva2VuIiwidmFsdWUiOiJZb3VyIGFjY2VzcyB0b2tlbiIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoidG9rZW5fc2VjcmV0IiwidmFsdWUiOiJZb3VyIHRva2VuIHNlY3JldCIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiYmVhcmVyX3Rva2VuIiwidmFsdWUiOm51bGwsImVuYWJsZWQiOnRydWV9XQ==) as well."]},{"cell_type":"markdown","id":"473d9e01","metadata":{"_cell_guid":"58c56e4a-0c31-4d1e-9190-6310f8752a4c","_uuid":"db993151-069a-459f-abef-49a975b46241","papermill":{"duration":0.005508,"end_time":"2023-05-02T08:06:54.114146","exception":false,"start_time":"2023-05-02T08:06:54.108638","status":"completed"},"tags":[]},"source":["In your main function, you can save this to a variable named data. Your main function should now have two variables one for url and one for data."]},{"cell_type":"markdown","id":"5c0c2be8","metadata":{"_cell_guid":"59b3ec1c-7b50-4ac4-a2b0-926f7c83a60b","_uuid":"908b1556-6b2c-4ac1-8fdb-0c77e9e0fcac","papermill":{"duration":0.005523,"end_time":"2023-05-02T08:06:54.125478","exception":false,"start_time":"2023-05-02T08:06:54.119955","status":"completed"},"tags":[]},"source":["To connect to the Twitter API, you’ll create a function called twitter_auth_and_connect where you’ll format the headers to pass in your bearer_token and url. At this point, this is where you connect to the Twitter API by using the request package to make a GET request."]},{"cell_type":"code","execution_count":5,"id":"0da46c51","metadata":{"_cell_guid":"3ce2ba69-20a5-4704-bb34-17cf91d1f6a8","_uuid":"0c7d4d78-129a-4150-86c7-5dfdcadb63fc","collapsed":false,"execution":{"iopub.execute_input":"2023-05-02T08:06:54.139077Z","iopub.status.busy":"2023-05-02T08:06:54.138615Z","iopub.status.idle":"2023-05-02T08:06:54.144734Z","shell.execute_reply":"2023-05-02T08:06:54.143537Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016306,"end_time":"2023-05-02T08:06:54.147573","exception":false,"start_time":"2023-05-02T08:06:54.131267","status":"completed"},"tags":[]},"outputs":[],"source":["def twitter_auth_and_connect(bearer_token, url):\n","    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n","    response = requests.request(\"GET\", url, headers=headers)\n","    return response.json()"]},{"cell_type":"markdown","id":"298ab759","metadata":{"_cell_guid":"4763067d-e1fe-4085-ac9b-29ce37cfc925","_uuid":"fde9af1c-9855-4c8b-9297-5ea62230ef02","papermill":{"duration":0.005577,"end_time":"2023-05-02T08:06:54.159093","exception":false,"start_time":"2023-05-02T08:06:54.153516","status":"completed"},"tags":[]},"source":["\n","\n","We will create a document-formatted file from Twitter instead of using the sample text in the Azure sample code, as the following code will be executed:"]},{"cell_type":"code","execution_count":6,"id":"d0d4987b","metadata":{"_cell_guid":"11c40a59-d40c-4813-a6a8-aec6b30c5fec","_uuid":"7e5b9742-ca56-4eae-9631-7db23ee536da","collapsed":false,"execution":{"iopub.execute_input":"2023-05-02T08:06:54.172671Z","iopub.status.busy":"2023-05-02T08:06:54.17224Z","iopub.status.idle":"2023-05-02T08:06:54.179047Z","shell.execute_reply":"2023-05-02T08:06:54.177536Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016661,"end_time":"2023-05-02T08:06:54.181648","exception":false,"start_time":"2023-05-02T08:06:54.164987","status":"completed"},"tags":[]},"outputs":[],"source":["def lang_data_shape(res_json):\n","    data_only = res_json[\"data\"]\n","    doc_start = '\"documents\": {}'.format(data_only)\n","    str_json = \"{\" + doc_start + \"}\"\n","    dump_doc = json.dumps(str_json)\n","    doc = json.loads(dump_doc)\n","    return ast.literal_eval(doc)"]},{"cell_type":"markdown","id":"b641155c","metadata":{"_cell_guid":"25c134e3-6227-4d30-9b31-75f608fe6fe4","_uuid":"66a3bab4-1684-47d2-8450-6582495a0ba7","papermill":{"duration":0.005503,"end_time":"2023-05-02T08:06:54.193142","exception":false,"start_time":"2023-05-02T08:06:54.187639","status":"completed"},"tags":[]},"source":["## Azure SDK for Python\n","\n","Azure provides two examples of sentiment analysis code: **Sentiment Analysis** and **Sentiment Analysis with Opinion Mining**. I use the first one ([**sample_analyze_sentiment.py**](https://github.com/hendro93/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_analyze_sentiment.py)) for the purposes of this textbook.\n","\n","To connect to Azure, you will need to format your data, by setting the environment variables with your own values, in a similar way to how you did with the Twitter API URL:\n","1. AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n","2. AZURE_LANGUAGE_KEY - your Language subscription key\n","\n"]},{"cell_type":"markdown","id":"a590cbf9","metadata":{"_cell_guid":"bf5467f8-9eac-4597-8011-44a44f666bb5","_uuid":"dad08fdd-a740-4379-9f68-bd99a0fb669a","papermill":{"duration":0.006649,"end_time":"2023-05-02T08:06:54.20563","exception":false,"start_time":"2023-05-02T08:06:54.198981","status":"completed"},"tags":[]},"source":["Obtaining sentiment scores\n","\n","Before you can use Azure’s endpoint for generating sentiment scores, you will need to combine the Tweet data with the data that contains the generated languages. You can use pandas to assist in this data conversion process. You can convert the json object with detected languages into a data frame. Since you only want the abbreviations of the language you can do a list comprehension to get the iso6391Name which contains abbreviations of languages. The iso6391Name is contained inside of a dictionary, which is inside of a list and the list is inside of the data frame with language data. You can also turn the Tweet data into a data frame and attach the abbreviation for the languages of your Tweets to that same data frame. From there, you can send that Tweet data into a JSON format."]},{"cell_type":"code","execution_count":7,"id":"a832e6d2","metadata":{"_cell_guid":"a554160f-b2f0-46cd-a0d2-2375a41b7820","_uuid":"9e0d0ed3-6146-4e94-8cef-3d2a06d3e4e4","collapsed":false,"execution":{"iopub.execute_input":"2023-05-02T08:06:54.2192Z","iopub.status.busy":"2023-05-02T08:06:54.218746Z","iopub.status.idle":"2023-05-02T08:06:55.59096Z","shell.execute_reply":"2023-05-02T08:06:55.588647Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.383434,"end_time":"2023-05-02T08:06:55.594898","exception":false,"start_time":"2023-05-02T08:06:54.211464","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","neutral\n","=======\n","@LonerMighty @Egi_nupe_ You speak melayu and don't know 'goreng goreng'? \n","The nasi goreng you eat, don't they cook it before frying?\n","\n","The progress you posted stated in step that you should cook. \n","And step 3 is where the frying comes in... \n","And that's typical means of making most 'goreng' in Malaysia\n","\n","\n","neutral\n","=======\n","RT @tastemadeid: Aneka resep nasi goreng. \n","Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n","\n","Video l…\n","\n","\n","neutral\n","=======\n","Mukbang nasi goreng Arab,mie goreng Arab,sosis 3 pcs,kcf,ampela ayam\n","\n","https://t.co/rNTteUPJgK\n","\n","\n","neutral\n","=======\n","Nasi + tahu gecok, terus di campur, terus di jadiin nasi goreng, gatau dah namanya apaan, kreasi paman tuh https://t.co/olNapHRl07\n","\n","\n","neutral\n","=======\n","RT @tastemadeid: Aneka resep nasi goreng. \n","Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n","\n","Video l…\n","\n","\n","negative\n","=======\n","RT @w_i_d_h_i: @rasjawa Dulu suka chicken pepper rice sejak harganya 29rb, tp setelah harganya tembus 60an rb kok rasanya terlalu mahal utk…\n","\n","\n","negative\n","=======\n","@rasjawa Dulu suka chicken pepper rice sejak harganya 29rb, tp setelah harganya tembus 60an rb kok rasanya terlalu mahal utk nasi goreng ayam doang\n","\n","\n","positive\n","=======\n","Nasi, telur goreng, ikan goreng &amp; sambal tempoyak.\n","\n","Lauk simple bcstaknak peningkan kepala. \n","I need to buy groceries too. \n","Tomorrow 👀\n","\n","#SinanjungSuriaEats https://t.co/2fk9X9MkzJ\n","\n","\n","neutral\n","=======\n","Nasi goreng https://t.co/znBxZ2pGVH\n","\n","\n","neutral\n","=======\n","Setelah nasi goreng, perkedel sekarang Tejo😭 https://t.co/AwaEmz3LPS\n"]}],"source":["def main():\n","    url = create_twitter_url()\n","    res_json = twitter_auth_and_connect(bearer_token, url)\n","    documents = lang_data_shape(res_json)\n","\n","    text_analytics_client = TextAnalyticsClient(\n","        endpoint=azure_endpoint, credential=AzureKeyCredential(azure_language_key)\n","    )\n","        \n","    result = text_analytics_client.analyze_sentiment(documents[\"documents\"], show_opinion_mining=False)\n","    doc_result = [doc for doc in result if not doc.is_error]\n","    \n","    for document in doc_result:\n","        print(\"\\n\")\n","        print(document.sentiment)\n","        print(\"=======\")\n","        for sentence in document.sentences:\n","            print(sentence.text)\n","            \n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","id":"0c143931","metadata":{"papermill":{"duration":0.005719,"end_time":"2023-05-02T08:06:55.606888","exception":false,"start_time":"2023-05-02T08:06:55.601169","status":"completed"},"tags":[]},"source":["Each tweet will be evaluated for sentiment, and the results will be displayed as follows:"]},{"cell_type":"markdown","id":"9cd3148a","metadata":{"papermill":{"duration":0.00583,"end_time":"2023-05-02T08:06:55.618774","exception":false,"start_time":"2023-05-02T08:06:55.612944","status":"completed"},"tags":[]},"source":["Azure intelligently draws conclusions based on positive, neutral, and negative sentiment score data from a text, so we no longer need to develop formulas to measure sentiment scores. The intelligent language detection feature of the Azure Sentiment Analysis service is another benefit. The most recent version has [**multilingual support**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/language-detection/language-support).\n"]},{"cell_type":"markdown","id":"f50ea25d","metadata":{"papermill":{"duration":0.005687,"end_time":"2023-05-02T08:06:55.630547","exception":false,"start_time":"2023-05-02T08:06:55.62486","status":"completed"},"tags":[]},"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":29.579193,"end_time":"2023-05-02T08:06:56.460315","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-05-02T08:06:26.881122","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}
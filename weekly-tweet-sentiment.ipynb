{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090ba08d",
   "metadata": {
    "_cell_guid": "84bda12a-134e-4a5b-ac2f-301fb66e376e",
    "_uuid": "5efa65ed-e03c-4f3f-a752-6fa8b046ed53",
    "papermill": {
     "duration": 0.006253,
     "end_time": "2023-05-02T07:39:18.178823",
     "exception": false,
     "start_time": "2023-05-02T07:39:18.172570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weekly Tweet Sentiment\n",
    "\n",
    "\n",
    "\n",
    "This notebook is based on Jessica Garson's step-by-step tutorial on her blog, [**How to analyze the sentiment of your own Tweets**](https://developer.twitter.com/en/blog/community/2020/how-to-analyze-the-sentiment-of-your-own-tweets). This is the [**complete code**](https://github.com/hendro93/weekly-tweet-sentiment).\n",
    "\n",
    "\n",
    "Setting up\n",
    "\n",
    "Before you can get started you will need to make sure you have the following:\n",
    "\n",
    "- Python 3 [**installed**](https://wiki.python.org/moin/BeginnersGuide/Download)\n",
    "- Twitter Developer account: if you don’t have one already, you can [**apply for one**].(https://developer.twitter.com/en/portal/dashboard)\n",
    "- [**A Twitter developer app**](https://developer.twitter.com/en/docs/apps/overview), which can be created in your Twitter developer account. \n",
    "- A [**bearer token**](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens) for your app\n",
    "- An account with Microsoft Azure’s [**Text Analytics Cognitive Service**](https://azure.microsoft.com/en-us/products/cognitive-services/text-analytics/) and an endpoint created. You can check out Microsoft’s [**quick start guide on how to call the Text Analytics API**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/overview).\n",
    "\n",
    "You will also need to install the library [**Requests**](https://requests.readthedocs.io/en/latest/). Requests will be used to make HTTP requests to the Twitter and Azure endpoints and pandas which is used to shape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861f2527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T07:39:18.191977Z",
     "iopub.status.busy": "2023-05-02T07:39:18.190564Z",
     "iopub.status.idle": "2023-05-02T07:39:31.680930Z",
     "shell.execute_reply": "2023-05-02T07:39:31.679657Z"
    },
    "papermill": {
     "duration": 13.500054,
     "end_time": "2023-05-02T07:39:31.683913",
     "exception": false,
     "start_time": "2023-05-02T07:39:18.183859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics\r\n",
      "  Downloading azure_ai_textanalytics-5.3.0b2-py3-none-any.whl (321 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting isodate<1.0.0,>=0.6.1\r\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting azure-core<2.0.0,>=1.24.0\r\n",
      "  Downloading azure_core-1.26.4-py3-none-any.whl (173 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting azure-common~=1.1\r\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from azure-ai-textanalytics) (4.4.0)\r\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.7/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.28.2)\r\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4)\r\n",
      "Installing collected packages: azure-common, isodate, azure-core, azure-ai-textanalytics\r\n",
      "Successfully installed azure-ai-textanalytics-5.3.0b2 azure-common-1.1.28 azure-core-1.26.4 isodate-0.6.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-textanalytics --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2644e2e1",
   "metadata": {
    "_cell_guid": "5f8228f2-bd56-4bcd-95ab-e6ffd7554529",
    "_uuid": "725e03d2-1180-4660-a46f-e7f7f92db669",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:39:31.697762Z",
     "iopub.status.busy": "2023-05-02T07:39:31.697270Z",
     "iopub.status.idle": "2023-05-02T07:39:32.479348Z",
     "shell.execute_reply": "2023-05-02T07:39:32.478091Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.792403,
     "end_time": "2023-05-02T07:39:32.482293",
     "exception": false,
     "start_time": "2023-05-02T07:39:31.689890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import ast\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "azure_endpoint = user_secrets.get_secret(\"AZURE_LANGUAGE_ENDPOINT\")\n",
    "azure_language_key = user_secrets.get_secret(\"AZURE_LANGUAGE_KEY\")\n",
    "bearer_token = user_secrets.get_secret(\"bearer_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60909b",
   "metadata": {
    "_cell_guid": "a09a4417-9841-41b0-91da-bab7d0b2b5c8",
    "_uuid": "683998eb-272b-4acd-8f1a-f177649493b1",
    "papermill": {
     "duration": 0.005285,
     "end_time": "2023-05-02T07:39:32.493256",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.487971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the URL\n",
    "\n",
    "Before you can connect the Twitter API, you’ll need to set up the URL to ensure it has the right fields so you get the right data back. You’ll first need to create a function called  create_twitter_url in this function you’ll declare a variable for your handle, you can replace jessicagarson with your own handle (I changed the query format to a keyword which can be anything so it doesn't have to be a Twitter ID). The max_results can be anywhere from 1 to 100 (but keep in mind that Azure Sentiment Analysis will only process a maximum of 10 records per request). If you are using a handle that would have more than 100 Tweets in a given week you may want to build in some logic to handle pagination or use a library such as [**searchtweets-labs**](https://github.com/twitterdev/search-tweets-python). The URL will need to be formatted to contain the max number of results and the query to say that you are looking for Tweets from a specific handle. You’ll return the formatted URL in a variable called url, since you will need it to make a get GET request later.\n",
    "\n",
    "Note the difference in how the most recent Twitter API urls are written!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf17feb",
   "metadata": {
    "_cell_guid": "69208944-04f4-46d2-8955-46b285e35dc9",
    "_uuid": "26f3ea5c-055d-4638-93f4-4c1013d8a901",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:39:32.506389Z",
     "iopub.status.busy": "2023-05-02T07:39:32.505503Z",
     "iopub.status.idle": "2023-05-02T07:39:32.512019Z",
     "shell.execute_reply": "2023-05-02T07:39:32.510705Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015816,
     "end_time": "2023-05-02T07:39:32.514447",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.498631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_twitter_url():\n",
    "    handle = \"nasi goreng\"\n",
    "    max_results = 10\n",
    "    mrf = \"max_results={}\".format(max_results)\n",
    "    q = \"query={}\".format(handle)\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?{}&{}\".format(\n",
    "        mrf, q\n",
    "    )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbf5b6",
   "metadata": {
    "_cell_guid": "ebd133b4-2c96-4456-81c7-d9a2ac86861b",
    "_uuid": "22beb0e6-936c-43c9-83b4-a65bc7e5edf0",
    "papermill": {
     "duration": 0.005439,
     "end_time": "2023-05-02T07:39:32.525630",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.520191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The URL you are creating is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6659d70d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T07:39:32.538742Z",
     "iopub.status.busy": "2023-05-02T07:39:32.538308Z",
     "iopub.status.idle": "2023-05-02T07:39:32.546615Z",
     "shell.execute_reply": "2023-05-02T07:39:32.545778Z"
    },
    "papermill": {
     "duration": 0.017706,
     "end_time": "2023-05-02T07:39:32.548978",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.531272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.twitter.com/2/tweets/search/recent?max_results=10&query=nasi goreng'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_twitter_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb8fe59",
   "metadata": {
    "_cell_guid": "cdfd905e-940d-42d9-b7a4-354bcb0f2a38",
    "_uuid": "0a5d197e-d40b-4e2f-a6aa-dc70d3226b18",
    "papermill": {
     "duration": 0.005521,
     "end_time": "2023-05-02T07:39:32.560325",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.554804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can adjust your query if you wanted to exclude retweets or Tweets that contain media. You can make adjustments to the data that is returned by the Twitter API by adding additional fields and expansions to your query. Using a REST client such as [**Postman**](https://www.postman.com/) or [**Insomnia**](https://insomnia.rest/) can be helpful for seeing what data you get back and making adjustments before you start writing code. There is [**a Postman collection for Labs endpoints**](https://app.getpostman.com/run-collection/c3c275c6ea02c49c3311#?env%5BTwitter%20Developer%20Labs%5D=W3sia2V5IjoiY29uc3VtZXJfa2V5IiwidmFsdWUiOiJZb3VyIGNvbnN1bWVyIGtleSIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiY29uc3VtZXJfc2VjcmV0IiwidmFsdWUiOiJZb3VyIGNvbnN1bWVyIHNlY3JldCIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiYWNjZXNzX3Rva2VuIiwidmFsdWUiOiJZb3VyIGFjY2VzcyB0b2tlbiIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoidG9rZW5fc2VjcmV0IiwidmFsdWUiOiJZb3VyIHRva2VuIHNlY3JldCIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiYmVhcmVyX3Rva2VuIiwidmFsdWUiOm51bGwsImVuYWJsZWQiOnRydWV9XQ==) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f01bc",
   "metadata": {
    "_cell_guid": "58c56e4a-0c31-4d1e-9190-6310f8752a4c",
    "_uuid": "db993151-069a-459f-abef-49a975b46241",
    "papermill": {
     "duration": 0.00674,
     "end_time": "2023-05-02T07:39:32.573309",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.566569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In your main function, you can save this to a variable named data. Your main function should now have two variables one for url and one for data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd763ac",
   "metadata": {
    "_cell_guid": "59b3ec1c-7b50-4ac4-a2b0-926f7c83a60b",
    "_uuid": "908b1556-6b2c-4ac1-8fdb-0c77e9e0fcac",
    "papermill": {
     "duration": 0.005734,
     "end_time": "2023-05-02T07:39:32.585355",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.579621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To connect to the Twitter API, you’ll create a function called twitter_auth_and_connect where you’ll format the headers to pass in your bearer_token and url. At this point, this is where you connect to the Twitter API by using the request package to make a GET request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bdb7f0c",
   "metadata": {
    "_cell_guid": "3ce2ba69-20a5-4704-bb34-17cf91d1f6a8",
    "_uuid": "0c7d4d78-129a-4150-86c7-5dfdcadb63fc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:39:32.600258Z",
     "iopub.status.busy": "2023-05-02T07:39:32.599800Z",
     "iopub.status.idle": "2023-05-02T07:39:32.607225Z",
     "shell.execute_reply": "2023-05-02T07:39:32.605376Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018241,
     "end_time": "2023-05-02T07:39:32.610173",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.591932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def twitter_auth_and_connect(bearer_token, url):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01265f",
   "metadata": {
    "_cell_guid": "4763067d-e1fe-4085-ac9b-29ce37cfc925",
    "_uuid": "fde9af1c-9855-4c8b-9297-5ea62230ef02",
    "papermill": {
     "duration": 0.005599,
     "end_time": "2023-05-02T07:39:32.622497",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.616898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "We will create a document-formatted file from Twitter instead of using the sample text in the Azure sample code, as the following code will be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c145aaa",
   "metadata": {
    "_cell_guid": "11c40a59-d40c-4813-a6a8-aec6b30c5fec",
    "_uuid": "7e5b9742-ca56-4eae-9631-7db23ee536da",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:39:32.637904Z",
     "iopub.status.busy": "2023-05-02T07:39:32.637442Z",
     "iopub.status.idle": "2023-05-02T07:39:32.644193Z",
     "shell.execute_reply": "2023-05-02T07:39:32.643092Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018466,
     "end_time": "2023-05-02T07:39:32.647795",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.629329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lang_data_shape(res_json):\n",
    "    data_only = res_json[\"data\"]\n",
    "    doc_start = '\"documents\": {}'.format(data_only)\n",
    "    str_json = \"{\" + doc_start + \"}\"\n",
    "    dump_doc = json.dumps(str_json)\n",
    "    doc = json.loads(dump_doc)\n",
    "    return ast.literal_eval(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239abcc",
   "metadata": {
    "_cell_guid": "25c134e3-6227-4d30-9b31-75f608fe6fe4",
    "_uuid": "66a3bab4-1684-47d2-8450-6582495a0ba7",
    "papermill": {
     "duration": 0.00626,
     "end_time": "2023-05-02T07:39:32.661601",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.655341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Azure SDK for Python\n",
    "\n",
    "Azure provides two examples of sentiment analysis code: **Sentiment Analysis** and **Sentiment Analysis with Opinion Mining**. I use the first one ([**sample_analyze_sentiment.py**](https://github.com/hendro93/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_analyze_sentiment.py)) for the purposes of this textbook.\n",
    "\n",
    "To connect to Azure, you will need to format your data, by setting the environment variables with your own values, in a similar way to how you did with the Twitter API URL:\n",
    "1. AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
    "2. AZURE_LANGUAGE_KEY - your Language subscription key\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3aa3b",
   "metadata": {
    "_cell_guid": "bf5467f8-9eac-4597-8011-44a44f666bb5",
    "_uuid": "dad08fdd-a740-4379-9f68-bd99a0fb669a",
    "papermill": {
     "duration": 0.007075,
     "end_time": "2023-05-02T07:39:32.674630",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.667555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Obtaining sentiment scores\n",
    "\n",
    "Before you can use Azure’s endpoint for generating sentiment scores, you will need to combine the Tweet data with the data that contains the generated languages. You can use pandas to assist in this data conversion process. You can convert the json object with detected languages into a data frame. Since you only want the abbreviations of the language you can do a list comprehension to get the iso6391Name which contains abbreviations of languages. The iso6391Name is contained inside of a dictionary, which is inside of a list and the list is inside of the data frame with language data. You can also turn the Tweet data into a data frame and attach the abbreviation for the languages of your Tweets to that same data frame. From there, you can send that Tweet data into a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae881fd",
   "metadata": {
    "_cell_guid": "a554160f-b2f0-46cd-a0d2-2375a41b7820",
    "_uuid": "9e0d0ed3-6146-4e94-8cef-3d2a06d3e4e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:39:32.688607Z",
     "iopub.status.busy": "2023-05-02T07:39:32.688154Z",
     "iopub.status.idle": "2023-05-02T07:39:33.965060Z",
     "shell.execute_reply": "2023-05-02T07:39:33.963726Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.287337,
     "end_time": "2023-05-02T07:39:33.967884",
     "exception": false,
     "start_time": "2023-05-02T07:39:32.680547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "RT @tastemadeid: Aneka resep nasi goreng. \n",
      "Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n",
      "\n",
      "Video l…\n",
      "\n",
      "\n",
      "negative\n",
      "=======\n",
      "Gaperlu ngerasa spesial, kamu manusia bukan nasi goreng-,-\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "RT @tastemadeid: Aneka resep nasi goreng. \n",
      "Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n",
      "\n",
      "Video l…\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "RT @tastemadeid: Aneka resep nasi goreng. \n",
      "Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n",
      "\n",
      "Video l…\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "RT @tastemadeid: Aneka resep nasi goreng. \n",
      "Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n",
      "\n",
      "Video l…\n",
      "\n",
      "\n",
      "positive\n",
      "=======\n",
      "Sebab tu jangan jeles dengan dak dak femes yang tunjuk kaya makan wagyu pergi healing bagai. \n",
      "Entah-entah lagi pokai dari kita yang makan nasi goreng tempe ni. \n",
      "https://t.co/WWFngMWSDF\n",
      "\n",
      "\n",
      "positive\n",
      "=======\n",
      "Harap nasi goreng makcik tu pedas btul smpai tak rasa lidah aku kjp lg. \n",
      "Bia ulcer ni makin besar belubang trus boleh ltk subang kt bibir. \n",
      "Bodo btul bila nak hilang entah. \n",
      "Makan nasi goreng dlm kreta tgok movie smbil tunggu baju basuh kt dobi. \n",
      "Tu nama die “me time” Yes 🤙🏻\n",
      "\n",
      "\n",
      "positive\n",
      "=======\n",
      "@Bang_Garr Wuiihh mantep tuh bang, aku td makan nasi liwet, goreng tahu sama lalapn🤤👍\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "@dfrian Nasi udah jadi nasi goreng\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "@anaqnya_mama Kyk nasi goreng yg jatuh kalau dr kerjauhan wkakakaa\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    url = create_twitter_url()\n",
    "    res_json = twitter_auth_and_connect(bearer_token, url)\n",
    "    documents = lang_data_shape(res_json)\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=azure_endpoint, credential=AzureKeyCredential(azure_language_key)\n",
    "    )\n",
    "        \n",
    "    result = text_analytics_client.analyze_sentiment(documents[\"documents\"], show_opinion_mining=False)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "    \n",
    "    for document in doc_result:\n",
    "        print(\"\\n\")\n",
    "        print(document.sentiment)\n",
    "        print(\"=======\")\n",
    "        for sentence in document.sentences:\n",
    "            print(sentence.text)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21e356",
   "metadata": {
    "papermill": {
     "duration": 0.005637,
     "end_time": "2023-05-02T07:39:33.979597",
     "exception": false,
     "start_time": "2023-05-02T07:39:33.973960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each tweet will be evaluated for sentiment, and the results will be displayed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861dc898",
   "metadata": {
    "papermill": {
     "duration": 0.005853,
     "end_time": "2023-05-02T07:39:33.991448",
     "exception": false,
     "start_time": "2023-05-02T07:39:33.985595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Azure intelligently draws conclusions based on positive, neutral, and negative sentiment score data from a text, so we no longer need to develop formulas to measure sentiment scores. The intelligent language detection feature of the Azure Sentiment Analysis service is another benefit. The most recent version has [**multilingual support**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/language-detection/language-support).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411140a8",
   "metadata": {
    "papermill": {
     "duration": 0.005656,
     "end_time": "2023-05-02T07:39:34.003052",
     "exception": false,
     "start_time": "2023-05-02T07:39:33.997396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.4599,
   "end_time": "2023-05-02T07:39:34.831574",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-02T07:39:06.371674",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0790fba6",
   "metadata": {
    "_cell_guid": "84bda12a-134e-4a5b-ac2f-301fb66e376e",
    "_uuid": "5efa65ed-e03c-4f3f-a752-6fa8b046ed53",
    "papermill": {
     "duration": 0.008036,
     "end_time": "2023-05-02T07:03:12.621696",
     "exception": false,
     "start_time": "2023-05-02T07:03:12.613660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weekly Tweet Sentiment\n",
    "\n",
    "\n",
    "\n",
    "This notebook is based on Jessica Garson's step-by-step tutorial on her blog, [**How to analyze the sentiment of your own Tweets**](https://developer.twitter.com/en/blog/community/2020/how-to-analyze-the-sentiment-of-your-own-tweets). This is the [**complete code**](https://github.com/hendro93/weekly-tweet-sentiment).\n",
    "\n",
    "\n",
    "Setting up\n",
    "\n",
    "Before you can get started you will need to make sure you have the following:\n",
    "\n",
    "- Python 3 [**installed**](https://wiki.python.org/moin/BeginnersGuide/Download)\n",
    "- Twitter Developer account: if you don‚Äôt have one already, you can [**apply for one**].(https://developer.twitter.com/en/portal/dashboard)\n",
    "- [**A Twitter developer app**](https://developer.twitter.com/en/docs/apps/overview), which can be created in your Twitter developer account. \n",
    "- A [**bearer token**](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens) for your app\n",
    "- An account with Microsoft Azure‚Äôs [**Text Analytics Cognitive Service**](https://azure.microsoft.com/en-us/products/cognitive-services/text-analytics/) and an endpoint created. You can check out Microsoft‚Äôs [**quick start guide on how to call the Text Analytics API**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/overview).\n",
    "\n",
    "You will need to create a dictionary for this project so in your terminal you can type the following, which will create a new directory for you and change from the directory you currently are to the new one you just created. You‚Äôll also create a new Python file and a [**YAML**](https://yaml.org/) configuration file that will be used to store your tokens and secrets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90ed22",
   "metadata": {
    "_cell_guid": "84bda12a-134e-4a5b-ac2f-301fb66e376e",
    "_uuid": "5efa65ed-e03c-4f3f-a752-6fa8b046ed53",
    "papermill": {
     "duration": 0.006451,
     "end_time": "2023-05-02T07:03:12.635108",
     "exception": false,
     "start_time": "2023-05-02T07:03:12.628657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weekly Tweet Sentiment\n",
    "\n",
    "\n",
    "\n",
    "This notebook is based on Jessica Garson's step-by-step tutorial on her blog, [**How to analyze the sentiment of your own Tweets**](https://developer.twitter.com/en/blog/community/2020/how-to-analyze-the-sentiment-of-your-own-tweets). This is the [**complete code**](https://github.com/hendro93/weekly-tweet-sentiment).\n",
    "\n",
    "\n",
    "Setting up\n",
    "\n",
    "Before you can get started you will need to make sure you have the following:\n",
    "\n",
    "- Python 3 [**installed**](https://wiki.python.org/moin/BeginnersGuide/Download)\n",
    "- Twitter Developer account: if you don‚Äôt have one already, you can [**apply for one**].(https://developer.twitter.com/en/portal/dashboard)\n",
    "- [**A Twitter developer app**](https://developer.twitter.com/en/docs/apps/overview), which can be created in your Twitter developer account. \n",
    "- A [**bearer token**](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens) for your app\n",
    "- An account with Microsoft Azure‚Äôs [**Text Analytics Cognitive Service**](https://azure.microsoft.com/en-us/products/cognitive-services/text-analytics/) and an endpoint created. You can check out Microsoft‚Äôs [**quick start guide on how to call the Text Analytics API**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/overview).\n",
    "\n",
    "You will need to create a dictionary for this project so in your terminal you can type the following, which will create a new directory for you and change from the directory you currently are to the new one you just created. You‚Äôll also create a new Python file and a [**YAML**](https://yaml.org/) configuration file that will be used to store your tokens and secrets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fcb21",
   "metadata": {
    "_cell_guid": "84bda12a-134e-4a5b-ac2f-301fb66e376e",
    "_uuid": "5efa65ed-e03c-4f3f-a752-6fa8b046ed53",
    "papermill": {
     "duration": 0.00639,
     "end_time": "2023-05-02T07:03:12.648193",
     "exception": false,
     "start_time": "2023-05-02T07:03:12.641803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weekly Tweet Sentiment\n",
    "\n",
    "\n",
    "\n",
    "This notebook is based on Jessica Garson's step-by-step tutorial on her blog, [**How to analyze the sentiment of your own Tweets**](https://developer.twitter.com/en/blog/community/2020/how-to-analyze-the-sentiment-of-your-own-tweets). This is the [**complete code**](https://github.com/hendro93/weekly-tweet-sentiment).\n",
    "\n",
    "\n",
    "Setting up\n",
    "\n",
    "Before you can get started you will need to make sure you have the following:\n",
    "\n",
    "- Python 3 [**installed**](https://wiki.python.org/moin/BeginnersGuide/Download)\n",
    "- Twitter Developer account: if you don‚Äôt have one already, you can [**apply for one**].(https://developer.twitter.com/en/portal/dashboard)\n",
    "- [**A Twitter developer app**](https://developer.twitter.com/en/docs/apps/overview), which can be created in your Twitter developer account. \n",
    "- A [**bearer token**](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens) for your app\n",
    "- An account with Microsoft Azure‚Äôs [**Text Analytics Cognitive Service**](https://azure.microsoft.com/en-us/products/cognitive-services/text-analytics/) and an endpoint created. You can check out Microsoft‚Äôs [**quick start guide on how to call the Text Analytics API**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/overview).\n",
    "\n",
    "You will need to create a dictionary for this project so in your terminal you can type the following, which will create a new directory for you and change from the directory you currently are to the new one you just created. You‚Äôll also create a new Python file and a [**YAML**](https://yaml.org/) configuration file that will be used to store your tokens and secrets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e333e",
   "metadata": {
    "_cell_guid": "84bda12a-134e-4a5b-ac2f-301fb66e376e",
    "_uuid": "5efa65ed-e03c-4f3f-a752-6fa8b046ed53",
    "papermill": {
     "duration": 0.006591,
     "end_time": "2023-05-02T07:03:12.661602",
     "exception": false,
     "start_time": "2023-05-02T07:03:12.655011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weekly Tweet Sentiment\n",
    "\n",
    "\n",
    "\n",
    "This notebook is based on Jessica Garson's step-by-step tutorial on her blog, [**How to analyze the sentiment of your own Tweets**](https://developer.twitter.com/en/blog/community/2020/how-to-analyze-the-sentiment-of-your-own-tweets). This is the [**complete code**](https://github.com/hendro93/weekly-tweet-sentiment).\n",
    "\n",
    "\n",
    "Setting up\n",
    "\n",
    "Before you can get started you will need to make sure you have the following:\n",
    "\n",
    "- Python 3 [**installed**](https://wiki.python.org/moin/BeginnersGuide/Download)\n",
    "- Twitter Developer account: if you don‚Äôt have one already, you can [**apply for one**].(https://developer.twitter.com/en/portal/dashboard)\n",
    "- [**A Twitter developer app**](https://developer.twitter.com/en/docs/apps/overview), which can be created in your Twitter developer account. \n",
    "- A [**bearer token**](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens) for your app\n",
    "- An account with Microsoft Azure‚Äôs [**Text Analytics Cognitive Service**](https://azure.microsoft.com/en-us/products/cognitive-services/text-analytics/) and an endpoint created. You can check out Microsoft‚Äôs [**quick start guide on how to call the Text Analytics API**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace10fa",
   "metadata": {
    "_cell_guid": "d790c556-835a-4114-9920-44b0b3f47988",
    "_uuid": "f8f57f3b-e392-4a5f-a7e2-d0f24c16f022",
    "papermill": {
     "duration": 0.006508,
     "end_time": "2023-05-02T07:03:12.674957",
     "exception": false,
     "start_time": "2023-05-02T07:03:12.668449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You will also need to install the library [**Requests**](https://requests.readthedocs.io/en/latest/). Requests will be used to make HTTP requests to the Twitter and Azure endpoints and pandas which is used to shape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d71d86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:12.690326Z",
     "iopub.status.busy": "2023-05-02T07:03:12.689933Z",
     "iopub.status.idle": "2023-05-02T07:03:25.997713Z",
     "shell.execute_reply": "2023-05-02T07:03:25.995943Z"
    },
    "papermill": {
     "duration": 13.318769,
     "end_time": "2023-05-02T07:03:26.000451",
     "exception": false,
     "start_time": "2023-05-02T07:03:12.681682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics\r\n",
      "  Downloading azure_ai_textanalytics-5.3.0b2-py3-none-any.whl (321 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting azure-common~=1.1\r\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\r\n",
      "Collecting azure-core<2.0.0,>=1.24.0\r\n",
      "  Downloading azure_core-1.26.4-py3-none-any.whl (173 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting isodate<1.0.0,>=0.6.1\r\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from azure-ai-textanalytics) (4.4.0)\r\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.7/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.28.2)\r\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4)\r\n",
      "Installing collected packages: azure-common, isodate, azure-core, azure-ai-textanalytics\r\n",
      "Successfully installed azure-ai-textanalytics-5.3.0b2 azure-common-1.1.28 azure-core-1.26.4 isodate-0.6.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-textanalytics --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e526ffb",
   "metadata": {
    "_cell_guid": "5f8228f2-bd56-4bcd-95ab-e6ffd7554529",
    "_uuid": "725e03d2-1180-4660-a46f-e7f7f92db669",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:26.018425Z",
     "iopub.status.busy": "2023-05-02T07:03:26.017643Z",
     "iopub.status.idle": "2023-05-02T07:03:26.761593Z",
     "shell.execute_reply": "2023-05-02T07:03:26.760153Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.756131,
     "end_time": "2023-05-02T07:03:26.764515",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.008384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import ast\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "azure_endpoint = user_secrets.get_secret(\"AZURE_LANGUAGE_ENDPOINT\")\n",
    "azure_language_key = user_secrets.get_secret(\"AZURE_LANGUAGE_KEY\")\n",
    "bearer_token = user_secrets.get_secret(\"bearer_token\")\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(endpoint=azure_endpoint, credential=AzureKeyCredential(azure_language_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11758b",
   "metadata": {
    "_cell_guid": "a09a4417-9841-41b0-91da-bab7d0b2b5c8",
    "_uuid": "683998eb-272b-4acd-8f1a-f177649493b1",
    "papermill": {
     "duration": 0.007374,
     "end_time": "2023-05-02T07:03:26.779427",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.772053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the URL\n",
    "\n",
    "Before you can connect the Twitter API, you‚Äôll need to set up the URL to ensure it has the right fields so you get the right data back. You‚Äôll first need to create a function called  create_twitter_url in this function you‚Äôll declare a variable for your handle, you can replace jessicagarson with your own handle (I changed the query format to a keyword which can be anything so it doesn't have to be a Twitter ID). The max_results can be anywhere from 1 to 100 (but keep in mind that Azure Sentiment Analysis will only process a maximum of 10 records per request). If you are using a handle that would have more than 100 Tweets in a given week you may want to build in some logic to handle pagination or use a library such as [**searchtweets-labs**](https://github.com/twitterdev/search-tweets-python). The URL will need to be formatted to contain the max number of results and the query to say that you are looking for Tweets from a specific handle. You‚Äôll return the formatted URL in a variable called url, since you will need it to make a get GET request later.\n",
    "\n",
    "Note the difference in how the most recent Twitter API urls are written!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4980b28",
   "metadata": {
    "_cell_guid": "69208944-04f4-46d2-8955-46b285e35dc9",
    "_uuid": "26f3ea5c-055d-4638-93f4-4c1013d8a901",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:26.797437Z",
     "iopub.status.busy": "2023-05-02T07:03:26.797037Z",
     "iopub.status.idle": "2023-05-02T07:03:26.803193Z",
     "shell.execute_reply": "2023-05-02T07:03:26.801986Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018286,
     "end_time": "2023-05-02T07:03:26.805485",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.787199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_twitter_url():\n",
    "    handle = \"nasi goreng\"\n",
    "    max_results = 10\n",
    "    mrf = \"max_results={}\".format(max_results)\n",
    "    q = \"query={}\".format(handle)\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?{}&{}\".format(\n",
    "        mrf, q\n",
    "    )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d8c4c",
   "metadata": {
    "_cell_guid": "ebd133b4-2c96-4456-81c7-d9a2ac86861b",
    "_uuid": "22beb0e6-936c-43c9-83b4-a65bc7e5edf0",
    "papermill": {
     "duration": 0.007586,
     "end_time": "2023-05-02T07:03:26.820809",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.813223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The URL you are creating is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb26cd",
   "metadata": {
    "papermill": {
     "duration": 0.00922,
     "end_time": "2023-05-02T07:03:26.838224",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.829004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "https://api.twitter.com/2/tweets/search/recent?max_results=100&query=from:jessicagarson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c37830",
   "metadata": {
    "_cell_guid": "cdfd905e-940d-42d9-b7a4-354bcb0f2a38",
    "_uuid": "0a5d197e-d40b-4e2f-a6aa-dc70d3226b18",
    "papermill": {
     "duration": 0.00734,
     "end_time": "2023-05-02T07:03:26.853101",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.845761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can adjust your query if you wanted to exclude retweets or Tweets that contain media. You can make adjustments to the data that is returned by the Twitter API by adding additional fields and expansions to your query. Using a REST client such as [**Postman**](https://www.postman.com/) or [**Insomnia**](https://insomnia.rest/) can be helpful for seeing what data you get back and making adjustments before you start writing code. There is [**a Postman collection for Labs endpoints**](https://app.getpostman.com/run-collection/c3c275c6ea02c49c3311#?env%5BTwitter%20Developer%20Labs%5D=W3sia2V5IjoiY29uc3VtZXJfa2V5IiwidmFsdWUiOiJZb3VyIGNvbnN1bWVyIGtleSIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiY29uc3VtZXJfc2VjcmV0IiwidmFsdWUiOiJZb3VyIGNvbnN1bWVyIHNlY3JldCIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiYWNjZXNzX3Rva2VuIiwidmFsdWUiOiJZb3VyIGFjY2VzcyB0b2tlbiIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoidG9rZW5fc2VjcmV0IiwidmFsdWUiOiJZb3VyIHRva2VuIHNlY3JldCIsImVuYWJsZWQiOnRydWV9LHsia2V5IjoiYmVhcmVyX3Rva2VuIiwidmFsdWUiOm51bGwsImVuYWJsZWQiOnRydWV9XQ==) as well.\n",
    "\n",
    "Setting up your main function\n",
    "\n",
    "At the bottom of the file, you can start to set up the main function that you will use to call all of the functions that you create. You can add the function you just created and call the function using an if __ name __ == \"__ main __\" statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60095d01",
   "metadata": {
    "_cell_guid": "5d352f23-f558-4612-858c-4186c7e6fac8",
    "_uuid": "f5c5cf9c-811c-4c27-89be-80a4f815c7e2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:26.871098Z",
     "iopub.status.busy": "2023-05-02T07:03:26.870385Z",
     "iopub.status.idle": "2023-05-02T07:03:26.875895Z",
     "shell.execute_reply": "2023-05-02T07:03:26.874959Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017815,
     "end_time": "2023-05-02T07:03:26.878607",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.860792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = create_twitter_url()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204f0ee",
   "metadata": {
    "_cell_guid": "58c56e4a-0c31-4d1e-9190-6310f8752a4c",
    "_uuid": "db993151-069a-459f-abef-49a975b46241",
    "papermill": {
     "duration": 0.007953,
     "end_time": "2023-05-02T07:03:26.894530",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.886577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In your main function, you can save this to a variable named data. Your main function should now have two variables one for url and one for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f95fcf",
   "metadata": {
    "_cell_guid": "eceffba7-f331-4c4b-b21f-7a296887a01c",
    "_uuid": "a0a0f585-47d7-4f33-b971-4c3265e1449f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:26.913424Z",
     "iopub.status.busy": "2023-05-02T07:03:26.912702Z",
     "iopub.status.idle": "2023-05-02T07:03:26.918111Z",
     "shell.execute_reply": "2023-05-02T07:03:26.917166Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018041,
     "end_time": "2023-05-02T07:03:26.920687",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.902646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = create_twitter_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96020f53",
   "metadata": {
    "_cell_guid": "59b3ec1c-7b50-4ac4-a2b0-926f7c83a60b",
    "_uuid": "908b1556-6b2c-4ac1-8fdb-0c77e9e0fcac",
    "papermill": {
     "duration": 0.007254,
     "end_time": "2023-05-02T07:03:26.935649",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.928395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To connect to the Twitter API, you‚Äôll create a function called twitter_auth_and_connect where you‚Äôll format the headers to pass in your bearer_token and url. At this point, this is where you connect to the Twitter API by using the request package to make a GET request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ff9c69",
   "metadata": {
    "_cell_guid": "3ce2ba69-20a5-4704-bb34-17cf91d1f6a8",
    "_uuid": "0c7d4d78-129a-4150-86c7-5dfdcadb63fc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:26.953752Z",
     "iopub.status.busy": "2023-05-02T07:03:26.953060Z",
     "iopub.status.idle": "2023-05-02T07:03:26.958867Z",
     "shell.execute_reply": "2023-05-02T07:03:26.958011Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0178,
     "end_time": "2023-05-02T07:03:26.961208",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.943408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def twitter_auth_and_connect(bearer_token, url):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c1fb3d8",
   "metadata": {
    "_cell_guid": "b1972935-5f12-4adc-b130-49aa073b7521",
    "_uuid": "06bd854d-ed56-4843-ba06-e97562c8641f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:26.979055Z",
     "iopub.status.busy": "2023-05-02T07:03:26.978352Z",
     "iopub.status.idle": "2023-05-02T07:03:26.983792Z",
     "shell.execute_reply": "2023-05-02T07:03:26.982926Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017273,
     "end_time": "2023-05-02T07:03:26.986077",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.968804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = create_twitter_url()\n",
    "    res_json = twitter_auth_and_connect(bearer_token, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef8070",
   "metadata": {
    "_cell_guid": "4763067d-e1fe-4085-ac9b-29ce37cfc925",
    "_uuid": "fde9af1c-9855-4c8b-9297-5ea62230ef02",
    "papermill": {
     "duration": 0.007168,
     "end_time": "2023-05-02T07:03:27.000718",
     "exception": false,
     "start_time": "2023-05-02T07:03:26.993550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction Azure SDK for Python\n",
    "\n",
    "Azure provides two examples of sentiment analysis code: **Sentiment Analysis** and **Sentiment Analysis with Opinion Mining**. I use the first one ([**sample_analyze_sentiment.py**](https://github.com/hendro93/azure-sdk-for-python/blob/main/sdk/textanalytics/azure-ai-textanalytics/samples/sample_analyze_sentiment.py)) for the purposes of this textbook.\n",
    "\n",
    "We will create a document-formatted file from Twitter instead of using the sample text in the Azure sample code, as the following code will be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b603459a",
   "metadata": {
    "_cell_guid": "11c40a59-d40c-4813-a6a8-aec6b30c5fec",
    "_uuid": "7e5b9742-ca56-4eae-9631-7db23ee536da",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:27.017777Z",
     "iopub.status.busy": "2023-05-02T07:03:27.017120Z",
     "iopub.status.idle": "2023-05-02T07:03:27.023480Z",
     "shell.execute_reply": "2023-05-02T07:03:27.022550Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017716,
     "end_time": "2023-05-02T07:03:27.025889",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.008173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lang_data_shape(res_json):\n",
    "    data_only = res_json[\"data\"]\n",
    "    doc_start = '\"documents\": {}'.format(data_only)\n",
    "    str_json = \"{\" + doc_start + \"}\"\n",
    "    dump_doc = json.dumps(str_json)\n",
    "    doc = json.loads(dump_doc)\n",
    "    return ast.literal_eval(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82915969",
   "metadata": {
    "_cell_guid": "25c134e3-6227-4d30-9b31-75f608fe6fe4",
    "_uuid": "66a3bab4-1684-47d2-8450-6582495a0ba7",
    "papermill": {
     "duration": 0.007308,
     "end_time": "2023-05-02T07:03:27.040856",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.033548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To connect to Azure, you will need to format your data, by setting the environment variables with your own values, in a similar way to how you did with the Twitter API URL:\n",
    "1. AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
    "2. AZURE_LANGUAGE_KEY - your Language subscription key\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda29dad",
   "metadata": {
    "_cell_guid": "bf5467f8-9eac-4597-8011-44a44f666bb5",
    "_uuid": "dad08fdd-a740-4379-9f68-bd99a0fb669a",
    "papermill": {
     "duration": 0.007153,
     "end_time": "2023-05-02T07:03:27.055522",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.048369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Obtaining sentiment scores\n",
    "\n",
    "Before you can use Azure‚Äôs endpoint for generating sentiment scores, you will need to combine the Tweet data with the data that contains the generated languages. You can use pandas to assist in this data conversion process. You can convert the json object with detected languages into a data frame. Since you only want the abbreviations of the language you can do a list comprehension to get the iso6391Name which contains abbreviations of languages. The iso6391Name is contained inside of a dictionary, which is inside of a list and the list is inside of the data frame with language data. You can also turn the Tweet data into a data frame and attach the abbreviation for the languages of your Tweets to that same data frame. From there, you can send that Tweet data into a JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364acf8e",
   "metadata": {
    "_cell_guid": "1278e82e-4523-45ce-916b-3b7d9efa9d39",
    "_uuid": "d4acf6fe-da35-4858-8788-db479c51e238",
    "papermill": {
     "duration": 0.007212,
     "end_time": "2023-05-02T07:03:27.070337",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.063125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Similarly to how you get the data into a dictionary format with the word documents: as the key in front of your payload to obtain the sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2461ed1",
   "metadata": {
    "_cell_guid": "7528d9b8-20de-4c34-bfcd-9e85fd95a22c",
    "_uuid": "c610b790-bffb-4819-a819-2108c3ba6d35",
    "papermill": {
     "duration": 0.007285,
     "end_time": "2023-05-02T07:03:27.085012",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.077727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, your data should be in the right format to call Azure‚Äôs sentiment endpoint. You can make a POST request to the sentiment endpoint you defined in the connect_to_azure function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f912382",
   "metadata": {
    "_cell_guid": "057bf933-b47b-4335-ae01-d09a55b901c5",
    "_uuid": "e9f274d3-c091-424b-850c-81a4cdf5982a",
    "papermill": {
     "duration": 0.007296,
     "end_time": "2023-05-02T07:03:27.099809",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.092513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The JSON response you will get returned should look similar to the payload below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79867372",
   "metadata": {
    "_cell_guid": "fd669480-5414-4b86-a70e-d5fccbff451f",
    "_uuid": "fbf6fcbb-3158-41a0-b7f0-4e12cd282fd6",
    "papermill": {
     "duration": 0.007197,
     "end_time": "2023-05-02T07:03:27.114624",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.107427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Your **documents**  should now look similar to the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3986d59f",
   "metadata": {
    "_cell_guid": "a554160f-b2f0-46cd-a0d2-2375a41b7820",
    "_uuid": "9e0d0ed3-6146-4e94-8cef-3d2a06d3e4e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:27.132106Z",
     "iopub.status.busy": "2023-05-02T07:03:27.131301Z",
     "iopub.status.idle": "2023-05-02T07:03:28.450690Z",
     "shell.execute_reply": "2023-05-02T07:03:28.448558Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.331567,
     "end_time": "2023-05-02T07:03:28.453732",
     "exception": false,
     "start_time": "2023-05-02T07:03:27.122165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "Dari barru sampe pangkep napa nda ad kuliat penjual nasi goreng, masuk maros baru dpt asw..\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "@RyuSakudora_VT Nasi goreng üòç Eh Akak orang Malay?\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "RT @tastemadeid: Aneka resep nasi goreng. \n",
      "Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n",
      "\n",
      "Video l‚Ä¶\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "RT @tastemadeid: Aneka resep nasi goreng. \n",
      "Ada yang gak pakai kecap, ada yang penuh bumbu sampai berwarna kemerahan, dan lain-lain.\n",
      "\n",
      "Video l‚Ä¶\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "@urryecil Nasi goreng\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "@alfieisonfire Nasi Goreng Chicken Chop\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "@dekbagass @yudhatripe @jogmfs Nasi goreng ijo top seller min\n",
      "\n",
      "\n",
      "positive\n",
      "=======\n",
      "RT @Haniff_drumb: Dikasih sarapan peju anget dari abang tukang nasi goreng, udh 1 bulan ga dikeluarin pejunyaü§§ Part 3 crott\n",
      "\n",
      "Gabung grup te‚Ä¶\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "umumu zelzel mie ayamku üòã km dapet pukis (peluk &amp;&amp; kiss) dari sharon nasi goreng basimu ini, mwah mwaahh üòªü´Çüíãüíãüå∑ https://t.co/A2zpD9wELJ\n",
      "\n",
      "\n",
      "neutral\n",
      "=======\n",
      "@FOODFESS2 Wkwkwwk kaya kang nasi goreng di deket rumah gua, tapi tetep nder gua beli lg soalnya porsinya bisa buat 2 orang dengan harga 15k ü§£\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    url = create_twitter_url()\n",
    "    res_json = twitter_auth_and_connect(bearer_token, url)\n",
    "    documents = lang_data_shape(res_json)\n",
    "\n",
    "        \n",
    "    result = text_analytics_client.analyze_sentiment(documents[\"documents\"], show_opinion_mining=False)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "    \n",
    "    for document in doc_result:\n",
    "        print(\"\\n\")\n",
    "        print(document.sentiment)\n",
    "        print(\"=======\")\n",
    "        for sentence in document.sentences:\n",
    "            print(sentence.text)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b48fc",
   "metadata": {
    "papermill": {
     "duration": 0.007542,
     "end_time": "2023-05-02T07:03:28.469414",
     "exception": false,
     "start_time": "2023-05-02T07:03:28.461872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each tweet will be evaluated for sentiment, and the results will be displayed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e708cf3",
   "metadata": {
    "papermill": {
     "duration": 0.007621,
     "end_time": "2023-05-02T07:03:28.484908",
     "exception": false,
     "start_time": "2023-05-02T07:03:28.477287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Azure intelligently draws conclusions based on positive, neutral, and negative sentiment score data from a text, so we no longer need to develop formulas to measure sentiment scores. The intelligent language detection feature of the Azure Sentiment Analysis service is another benefit. The most recent version has [**multilingual support**](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/language-detection/language-support).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ec1ab",
   "metadata": {
    "papermill": {
     "duration": 0.007503,
     "end_time": "2023-05-02T07:03:28.500203",
     "exception": false,
     "start_time": "2023-05-02T07:03:28.492700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.705965,
   "end_time": "2023-05-02T07:03:29.334147",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-02T07:03:00.628182",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
